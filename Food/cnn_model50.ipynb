{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "#convert pooling features space to large feature vector for fully\n",
    "#connected layer \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ML\\Anaconda3\\envs\\New\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ML\\Anaconda3\\envs\\New\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size = (3, 3), activation='relu', input_shape=(128,128, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(96, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(101, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = None,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 101000 images belonging to 101 classes.\n",
      "{'apple_pie': 0, 'baby_back_ribs': 1, 'baklava': 2, 'beef_carpaccio': 3, 'beef_tartare': 4, 'beet_salad': 5, 'beignets': 6, 'bibimbap': 7, 'bread_pudding': 8, 'breakfast_burrito': 9, 'bruschetta': 10, 'caesar_salad': 11, 'cannoli': 12, 'caprese_salad': 13, 'carrot_cake': 14, 'ceviche': 15, 'cheese_plate': 16, 'cheesecake': 17, 'chicken_curry': 18, 'chicken_quesadilla': 19, 'chicken_wings': 20, 'chocolate_cake': 21, 'chocolate_mousse': 22, 'churros': 23, 'clam_chowder': 24, 'club_sandwich': 25, 'crab_cakes': 26, 'creme_brulee': 27, 'croque_madame': 28, 'cup_cakes': 29, 'deviled_eggs': 30, 'donuts': 31, 'dumplings': 32, 'edamame': 33, 'eggs_benedict': 34, 'escargots': 35, 'falafel': 36, 'filet_mignon': 37, 'fish_and_chips': 38, 'foie_gras': 39, 'french_fries': 40, 'french_onion_soup': 41, 'french_toast': 42, 'fried_calamari': 43, 'fried_rice': 44, 'frozen_yogurt': 45, 'garlic_bread': 46, 'gnocchi': 47, 'greek_salad': 48, 'grilled_cheese_sandwich': 49, 'grilled_salmon': 50, 'guacamole': 51, 'gyoza': 52, 'hamburger': 53, 'hot_and_sour_soup': 54, 'hot_dog': 55, 'huevos_rancheros': 56, 'hummus': 57, 'ice_cream': 58, 'lasagna': 59, 'lobster_bisque': 60, 'lobster_roll_sandwich': 61, 'macaroni_and_cheese': 62, 'macarons': 63, 'miso_soup': 64, 'mussels': 65, 'nachos': 66, 'omelette': 67, 'onion_rings': 68, 'oysters': 69, 'pad_thai': 70, 'paella': 71, 'pancakes': 72, 'panna_cotta': 73, 'peking_duck': 74, 'pho': 75, 'pizza': 76, 'pork_chop': 77, 'poutine': 78, 'prime_rib': 79, 'pulled_pork_sandwich': 80, 'ramen': 81, 'ravioli': 82, 'red_velvet_cake': 83, 'risotto': 84, 'samosa': 85, 'sashimi': 86, 'scallops': 87, 'seaweed_salad': 88, 'shrimp_and_grits': 89, 'spaghetti_bolognese': 90, 'spaghetti_carbonara': 91, 'spring_rolls': 92, 'steak': 93, 'strawberry_shortcake': 94, 'sushi': 95, 'tacos': 96, 'takoyaki': 97, 'tiramisu': 98, 'tuna_tartare': 99, 'waffles': 100}\n"
     ]
    }
   ],
   "source": [
    "training_set = train_datagen.flow_from_directory(r'E:\\Dataset\\Food\\SK_Food\\images',\n",
    "                                                 target_size = (128, 128),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical')\n",
    "#print(test_datagen);\n",
    "labels = (training_set.class_indices)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ML\\Anaconda3\\envs\\New\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "375/375 [==============================] - 959s 3s/step - loss: 4.6283 - acc: 0.0233\n",
      "Epoch 2/50\n",
      "375/375 [==============================] - 1004s 3s/step - loss: 4.3778 - acc: 0.0462\n",
      "Epoch 3/50\n",
      "375/375 [==============================] - 1022s 3s/step - loss: 4.2497 - acc: 0.0637\n",
      "Epoch 4/50\n",
      "375/375 [==============================] - 922s 2s/step - loss: 4.1414 - acc: 0.0749\n",
      "Epoch 5/50\n",
      "375/375 [==============================] - 913s 2s/step - loss: 4.0535 - acc: 0.0877\n",
      "Epoch 6/50\n",
      "375/375 [==============================] - 894s 2s/step - loss: 3.9665 - acc: 0.0981\n",
      "Epoch 7/50\n",
      "375/375 [==============================] - 899s 2s/step - loss: 3.9111 - acc: 0.1052\n",
      "Epoch 8/50\n",
      "375/375 [==============================] - 897s 2s/step - loss: 3.8503 - acc: 0.1101\n",
      "Epoch 9/50\n",
      "375/375 [==============================] - 894s 2s/step - loss: 3.7787 - acc: 0.1275\n",
      "Epoch 10/50\n",
      "375/375 [==============================] - 896s 2s/step - loss: 3.7399 - acc: 0.1353\n",
      "Epoch 11/50\n",
      "375/375 [==============================] - 898s 2s/step - loss: 3.7022 - acc: 0.1396\n",
      "Epoch 12/50\n",
      "375/375 [==============================] - 900s 2s/step - loss: 3.6439 - acc: 0.1519\n",
      "Epoch 13/50\n",
      "375/375 [==============================] - 903s 2s/step - loss: 3.5879 - acc: 0.1570\n",
      "Epoch 14/50\n",
      "375/375 [==============================] - 902s 2s/step - loss: 3.5705 - acc: 0.1650\n",
      "Epoch 15/50\n",
      "375/375 [==============================] - 917s 2s/step - loss: 3.5282 - acc: 0.1706\n",
      "Epoch 16/50\n",
      "375/375 [==============================] - 918s 2s/step - loss: 3.4738 - acc: 0.1779\n",
      "Epoch 17/50\n",
      "375/375 [==============================] - 901s 2s/step - loss: 3.4191 - acc: 0.1883\n",
      "Epoch 18/50\n",
      "375/375 [==============================] - 900s 2s/step - loss: 3.3466 - acc: 0.1993\n",
      "Epoch 19/50\n",
      "375/375 [==============================] - 898s 2s/step - loss: 3.3471 - acc: 0.2020\n",
      "Epoch 20/50\n",
      "375/375 [==============================] - 905s 2s/step - loss: 3.3329 - acc: 0.2061\n",
      "Epoch 21/50\n",
      "375/375 [==============================] - 910s 2s/step - loss: 3.3089 - acc: 0.2062\n",
      "Epoch 22/50\n",
      "375/375 [==============================] - 909s 2s/step - loss: 3.2803 - acc: 0.2157\n",
      "Epoch 23/50\n",
      "375/375 [==============================] - 903s 2s/step - loss: 3.2543 - acc: 0.2149\n",
      "Epoch 24/50\n",
      "375/375 [==============================] - 935s 2s/step - loss: 3.2186 - acc: 0.2305\n",
      "Epoch 25/50\n",
      "375/375 [==============================] - 929s 2s/step - loss: 3.2027 - acc: 0.2298\n",
      "Epoch 26/50\n",
      "375/375 [==============================] - 911s 2s/step - loss: 3.1785 - acc: 0.2337\n",
      "Epoch 27/50\n",
      "375/375 [==============================] - 920s 2s/step - loss: 3.1482 - acc: 0.2403\n",
      "Epoch 28/50\n",
      "375/375 [==============================] - 914s 2s/step - loss: 3.1104 - acc: 0.2520\n",
      "Epoch 29/50\n",
      "375/375 [==============================] - 927s 2s/step - loss: 3.1263 - acc: 0.2448\n",
      "Epoch 30/50\n",
      "375/375 [==============================] - 909s 2s/step - loss: 3.1144 - acc: 0.2445\n",
      "Epoch 31/50\n",
      "375/375 [==============================] - 911s 2s/step - loss: 3.1011 - acc: 0.2477\n",
      "Epoch 32/50\n",
      "375/375 [==============================] - 908s 2s/step - loss: 3.0804 - acc: 0.2511\n",
      "Epoch 33/50\n",
      "375/375 [==============================] - 907s 2s/step - loss: 3.0595 - acc: 0.2572\n",
      "Epoch 34/50\n",
      "375/375 [==============================] - 902s 2s/step - loss: 3.0718 - acc: 0.2559\n",
      "Epoch 35/50\n",
      "375/375 [==============================] - 901s 2s/step - loss: 3.0007 - acc: 0.2682\n",
      "Epoch 36/50\n",
      "375/375 [==============================] - 906s 2s/step - loss: 3.0098 - acc: 0.2734\n",
      "Epoch 37/50\n",
      "375/375 [==============================] - 909s 2s/step - loss: 2.9922 - acc: 0.2685\n",
      "Epoch 38/50\n",
      "375/375 [==============================] - 947s 3s/step - loss: 3.0044 - acc: 0.2653\n",
      "Epoch 39/50\n",
      "375/375 [==============================] - 985s 3s/step - loss: 2.9641 - acc: 0.2717\n",
      "Epoch 40/50\n",
      "205/375 [===============>..............] - ETA: 7:17 - loss: 3.0095 - acc: 0.2712"
     ]
    }
   ],
   "source": [
    "model.fit_generator(training_set,\n",
    "                         steps_per_epoch = 375,\n",
    "                         epochs = 50)\n",
    "                         #validation_data = test_set,\n",
    "                         #validation_steps = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 - Making new predictions\n",
    "\n",
    "model_json=model.to_json()\n",
    "with open(\"model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "    model.save_weights(\"model2.h5\")\n",
    "    print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
